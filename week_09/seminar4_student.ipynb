{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b44463",
   "metadata": {},
   "source": [
    "# Seminar: Diphone Synthersis\n",
    "At this seminar we will construct the simpliest possible synthesis - diphone model.\n",
    "<img src=\"concat-scheme.png\">\n",
    "We will use part of the LJSpeech dataset.\n",
    "Your task will be to design search and concatenation of the units.\n",
    "Preprocessor stages are already performed for the test samples (and it'll be your home assignment to create a small g2p for CMU english phoneset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d91fd",
   "metadata": {},
   "source": [
    "## Alignment\n",
    "The first and very import part in the data preparation is alignment: we need to determine the timings of phonemes our utterance consists of.\n",
    "Even the concatenative syntheses are not used today in prod alignment is still an important phase for upsampling-based parametric acoustic models (e.g. fastspeech)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51791694",
   "metadata": {},
   "source": [
    "### Motreal Force Aligner\n",
    "To process audio we will use MFA.\n",
    "\n",
    "At the alignment stage we launch xent-trained TDNN ASR system with fixed text on the output and try to determine the most probable phonemes positions in the timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09/wavs_need.txt\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09/test_phones.txt\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09/fallback_rules.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57470f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile install_mfa.sh\n",
    "#!/bin/bash\n",
    "\n",
    "## a script to install Montreal Forced Aligner (MFA)\n",
    "\n",
    "root_dir=${1:-/tmp/mfa}\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# download miniconda3\n",
    "wget -q --show-progress https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh -b -p $root_dir/miniconda3 -f\n",
    "\n",
    "# create py38 env\n",
    "$root_dir/miniconda3/bin/conda create -n aligner -c conda-forge openblas python=3.8 openfst pynini ngram baumwelch -y\n",
    "source $root_dir/miniconda3/bin/activate aligner\n",
    "\n",
    "# install mfa, download kaldi\n",
    "pip install montreal-forced-aligner praat-textgrids  # install requirements\n",
    "pip install git+https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner.git # install latest updates\n",
    "\n",
    "mfa thirdparty download\n",
    "\n",
    "echo -e \"\\n======== DONE ==========\"\n",
    "echo -e \"\\nTo activate MFA, run: source $root_dir/miniconda3/bin/activate aligner\"\n",
    "echo -e \"\\nTo delete MFA, run: rm -rf $root_dir\"\n",
    "echo -e \"\\nSee: https://montreal-forced-aligner.readthedocs.io/en/latest/aligning.html to know how to use MFA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install mfa\n",
    "INSTALL_DIR=\"/tmp/mfa\" # path to install directory\n",
    "\n",
    "!bash ./install_mfa.sh {INSTALL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d25c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source {INSTALL_DIR}/miniconda3/bin/activate aligner; mfa align --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d71488",
   "metadata": {},
   "source": [
    "### LJSpeech data subset\n",
    "Here we will download the dataset.\n",
    "However we don't need the whole LJSpeech for diphone synthesis (and it will be processed for quite a while).\n",
    "Here we will take about 1/10 of the dataset. That's more than enough for diphone TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957caca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"download and unpack ljs dataset\"\n",
    "!mkdir -p ./ljs; cd ./ljs; wget -q --show-progress https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "!cd ./ljs; tar xjf LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178dd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need sox to convert audio to 16kHz (the format alignment works with)\n",
    "!sudo apt install -q -y sox\n",
    "!sudo apt install -q -y libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./wav\n",
    "!cat wavs_need.txt | xargs -I F -P 30 sox --norm=-3 ./ljs/LJSpeech-1.1/wavs/F.wav -r 16k -c 1 ./wav/F.wav\n",
    "!echo \"Number of clips\" $(ls ./wav/ | wc -l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb74eb4",
   "metadata": {},
   "source": [
    "It should be 1273 clips here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea606be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wavs_need.txt') as ifile:\n",
    "    wavs_need = {l.strip() for l in ifile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03971e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata to transcripts\n",
    "lines = open('./ljs/LJSpeech-1.1/metadata.csv', 'r').readlines()\n",
    "for line in lines:\n",
    "    fn, _, transcript = line.strip().split('|')\n",
    "    if fn in wavs_need:\n",
    "        with open(f'./wav/{fn}.txt', 'w') as ofile:\n",
    "            ofile.write(transcript)\n",
    "\n",
    "!echo \"Number of transcripts\" $(ls ./wav/*.txt | wc -l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9014479",
   "metadata": {},
   "source": [
    "Let's download the artifacts for alignment.\n",
    "\n",
    "For phoneme ASR we need acoustic model and lexicon (mapping word=>phonemes) made by some other g2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d711e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q --show-progress https://github.com/MontrealCorpusTools/mfa-models/raw/master/acoustic/english.zip\n",
    "!wget -q --show-progress http://www.openslr.org/resources/11/librispeech-lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a88db",
   "metadata": {},
   "source": [
    "Finally, we come to the alignment.\n",
    "\n",
    "It will take about 15-17 min for our subset to be aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source {INSTALL_DIR}/miniconda3/bin/activate aligner; \\\n",
    "mfa align -t ./temp -c -j 4 ./wav librispeech-lexicon.txt ./english.zip ./ljs_aligned\n",
    "!echo \"See output files at ./ljs_aligned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ljs_aligned/|wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "from IPython.core.display import display\n",
    "\n",
    "def display_audio(data):\n",
    "    display(IPython.display.Audio(data, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d104f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import textgrids\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3edb28",
   "metadata": {},
   "source": [
    "Alignment outputs are textgrids - and xml-like structure with layers for phonemes and words (with timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = {f.split(\"/\")[-1].split(\".\")[0][4:]: textgrids.TextGrid(f) for f in glob.iglob('ljs_aligned/*')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = {f.split(\"/\")[-1].split(\".\")[0]: wavfile.read(f)[1] for f in glob.iglob('./ljs/LJSpeech-1.1/wavs/*.wav')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb91a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "allphones = {\n",
    "    ph.text for grid in alignment.values() for ph in grid[\"phones\"]\n",
    "}\n",
    "# let's exclude special symbols: silence, spoken noise, non-spoken noise\n",
    "allphones = {ph for ph in allphones if ph == ph.upper()}\n",
    "assert len(allphones) == 69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9db0f0",
   "metadata": {},
   "source": [
    "Here your part begins:\n",
    "You need to create `diphone index` - mapping structure that will allow you to find original utterance and position in it by diphone text id.\n",
    "\n",
    "E.g.:\n",
    "`index[(PH1, PH2)] -> (utt_id, phoneme_index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7579cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diphone_index = dict()\n",
    "# !!!!!!!!!!!!!!!!!!!!!!#\n",
    "# INSERT YOUR CODE HERE #\n",
    "# !!!!!!!!!!!!!!!!!!!!!!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check yourself\n",
    "for a, b in [('AH0', 'P'), ('P', 'AH0'), ('AH0', 'L')]:\n",
    "    k, i = diphone_index[(a,b)]\n",
    "    assert a == alignment[k]['phones'][i].text\n",
    "    assert b == alignment[k]['phones'][i+1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99a513",
   "metadata": {},
   "source": [
    "In concat TTS you sometimes don't have all the diphones presented\n",
    "If it's not very frequent ones it's not a trouble\n",
    "But we need to provide some mechanism to replace missing units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fallback_rules.txt\") as ifile:\n",
    "    lines = [l.strip().split() for l in ifile]\n",
    "    fallback_rules = {l[0]: l[1:] for l in lines}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa781e",
   "metadata": {},
   "source": [
    "In the dict `fallback_rules` lie possible replacement for all the phones\n",
    "(different replacements in order of similarity).\n",
    "\n",
    "E.g. `a stressed` -> `a unstressed`  | `o stressed` | `o unstressed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5731307",
   "metadata": {},
   "source": [
    "Here is also some work for you:\n",
    "You need to create diphone fallbacks from the phoneme ones:\n",
    "\n",
    "`diphone_fallbacks[(Ph1, Ph2)] -> (some_other_pair_of_phones_presented_in_dataset)`\n",
    "\n",
    "and also, if `diphone_fallbacks[(a, b)] = c, d` then:\n",
    "* c = a or\n",
    "* c $\\in$ fallback_rules[a] and/or\n",
    "* d = b or\n",
    "* d $\\in$ fallback_rules[d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "diphone_fallbacks = dict()\n",
    "# !!!!!!!!!!!!!!!!!!!!!!#\n",
    "# INSERT YOUR CODE HERE #\n",
    "# !!!!!!!!!!!!!!!!!!!!!!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check yourself\n",
    "for a, b in [('Z', 'Z'), ('Z', 'AY1'), ('Z', 'EY0')]:\n",
    "    assert (a, b) in diphone_fallbacks\n",
    "    r1, r2 = diphone_fallbacks[(a, b)]\n",
    "    assert r1 in fallback_rules[a] or r1 == a\n",
    "    assert r2 in fallback_rules[b] or r2 == b\n",
    "    assert r1 != a or r2 != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helping constants\n",
    "SAMPLE_RATE = 22050\n",
    "WAV_TYPE = np.int16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd273d2",
   "metadata": {},
   "source": [
    "Little DSP related to concatenative synthesis:\n",
    "\n",
    "to prevent disturbing \"clicking\" sound (difference in volume) when concatenating fragments from different utterances we need to perform `cross-fade` - smoothing at concatenation point\n",
    "\n",
    "If we concatenate $wav_1$ and $wav_2$ at some points $M_1$ and $M_2$ corrispondively we perform crossfade with overlap of $2 V$:\n",
    "\n",
    "$$\\forall i \\in [-V; V]:~output[M_1+i] = (1-\\alpha) \\cdot wav_1[M_1+i] + \\alpha \\cdot wav_2[M_2+i]$$\n",
    "Where $$\\alpha = \\frac{i+V}{2 V}$$\n",
    "\n",
    "And for $i < -V:~ output[M_1+i] = wav_1[M_1+i]$\n",
    "\n",
    "for $i > V:~output[M_1+i] = wav_2[M_2+i]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f362f4",
   "metadata": {},
   "source": [
    "But it is not ok if the overlapping comes outside the concatenation phoneme.\n",
    "\n",
    "So, if junction phoneme starts and ends at positions $B_1$ and $E_1$ (the first wav) and $B_2$ and $E_2$ (the second one)\n",
    "the extact formula for overlapping zone will be:\n",
    "$$\\forall i \\in [-L; R]:~output[M_1+i] = (1-\\alpha) \\cdot wav_1[M_1+i] + \\alpha \\cdot wav_2[M_2+i]$$\n",
    "Where:\n",
    "$$\\alpha = \\frac{i+L}{L+R},~L = min(M_1-B_1, M_2 - B_2, V), ~R = min(E_1-M_1, E_2-M_2, V)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossfade(lcenter, ldata, rcenter, rdata, halfoverlap):\n",
    "    \"\"\"\n",
    "    ldata, rdata - 1d numpy array only with junction phoneme (so, B1 = 0, E1 = ldata.shape[0])\n",
    "    lcenter = M1\n",
    "    rcenter = M2\n",
    "    \n",
    "    it is better to return the concatenated version of the junction phoneme (as numpy data)\n",
    "    \"\"\"\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!#\n",
    "    # INSERT YOUR CODE HERE #\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b169a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(k, i):\n",
    "    phoneme = alignment[k]['phones'][i]\n",
    "    left = phoneme.xmin\n",
    "    right = phoneme.xmax\n",
    "    center = (left+right) * .5\n",
    "    \n",
    "    left = int(left * SAMPLE_RATE)\n",
    "    center = int(center * SAMPLE_RATE)\n",
    "    right = int(right * SAMPLE_RATE)\n",
    "    return center - left, wavs[k][left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffada2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check yourself\n",
    "cf = crossfade(*get_data('LJ050-0241', 3), *get_data('LJ038-0067', 56), 300)\n",
    "assert np.abs(cf.shape[0] - 1764) < 10\n",
    "assert np.abs(cf.mean() - 11) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HALF_OVERLAP_CROSSFADE = 300\n",
    "\n",
    "def synthesize(phonemes):\n",
    "    diphones = []\n",
    "    for ph1, ph2 in zip(phonemes[:-1], phonemes[1:]):\n",
    "        diphone = (ph1, ph2)\n",
    "        if diphone in diphone_index:\n",
    "            k, i = diphone_index[diphone]\n",
    "        else:\n",
    "            k, i = diphone_index[diphone_fallbacks[diphone]]\n",
    "            \n",
    "        diphones.append((get_data(k, i), get_data(k, i+1)))\n",
    "    output = []\n",
    "    \n",
    "    # Here you need to construct the result utterance with crossfades\n",
    "    # NB: border (the first and the last phonemes does not require any crossfade and could be just copied)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!#\n",
    "    # INSERT YOUR CODE HERE #\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!#\n",
    "    # need to return wav as 1d numpy array of type WAV_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dca359",
   "metadata": {},
   "source": [
    "Check youself:\n",
    "\n",
    "If everything was correct, you should hear 'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bd120",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_audio(synthesize(['HH', 'AH0', 'L', 'OW1', 'W', 'ER1', 'L', 'D']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load additional test texts\n",
    "with open(\"test_phones.txt\") as ifile:\n",
    "    test_phones = []\n",
    "    for l in ifile:\n",
    "        test_phones.append(l.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291e69a",
   "metadata": {},
   "source": [
    "Here should a little part of the GLADOS song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6709c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "pause = np.zeros([int(0.1 * SAMPLE_RATE)], dtype=WAV_TYPE)\n",
    "for test in test_phones:\n",
    "    output.append(synthesize(test))\n",
    "    output.append(pause)\n",
    "    \n",
    "display_audio(np.concatenate(output[:-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}